{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks 2\n",
    "\n",
    "## Forward and Backward Propagation Methods\n",
    "\n",
    "Last time you built a neural network that was able to create multiple neurons per layer and multiple layers in the network.  The consolidated code from last time is given below which we will add to through this lesson to build the key algorithms which make all neural networks operate: Forward Propagation and Backward Propagation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# From Lesson 1\n",
    "def print_network(network):\n",
    "    \"\"\"\n",
    "    Prints the current neural network weights and results for each neuron\n",
    "    \"\"\"\n",
    "    print(\"Neuron_Index: [Weights] => Neuron_Output\\n\")\n",
    "\n",
    "    for layer in network.Layers:\n",
    "        if layer.Layer_Index == 0:\n",
    "            layer_type = \"Input\"\n",
    "        elif layer.Layer_Index == (len(network.Layers) - 1):\n",
    "            layer_type = \"Output\"\n",
    "        else:\n",
    "            layer_type = \"Hidden\"\n",
    "        print(\"Layer {0} - {1}\".format(layer.Layer_Index, layer_type))\n",
    "        for neuron in layer.Neurons:\n",
    "            print(\"   Neuron {0}: {1} => {2}\".format(neuron.Neuron_Index, neuron.Weights, neuron.Result))\n",
    "\n",
    "\n",
    "class Network:\n",
    "\n",
    "    def __init__(self, description, initialization_value, learning_rate):\n",
    "        self.Layers = []\n",
    "        # Initialize the input layer\n",
    "        self.Layers.append(Layer(0, description[0], 0, initialization_value, learning_rate))\n",
    "        # Initialize the subsequent layers\n",
    "        for i in range(1, len(description)):\n",
    "            self.Layers.append(Layer(description[i - 1], description[i], i, initialization_value, learning_rate))\n",
    "        self.Learning_Rate = learning_rate\n",
    "\n",
    "    def set_weights(self, layer_index, neuron_index, weights):\n",
    "        self.Layers[layer_index].set_weights(neuron_index, weights)\n",
    "\n",
    "\n",
    "class Layer:\n",
    "\n",
    "    def __init__(self, number_neurons_previous_layer, number_neurons, layer_index, initialization_value, learning_rate):\n",
    "        self.Neurons = []\n",
    "        self.Layer_Index = layer_index\n",
    "        for i in range(number_neurons):\n",
    "            self.Neurons.append(Neuron(number_neurons_previous_layer, i, initialization_value, learning_rate))\n",
    "\n",
    "    def set_weights(self, neuron_index, weights):\n",
    "        self.Neurons[neuron_index].set_weights(weights)\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "\n",
    "    def __init__(self, number_neurons_in_previous_layer, neuron_index, initialization_value, learning_rate):\n",
    "        self.Weights = []\n",
    "        self.Result = 0\n",
    "        # self.learning_rate = learning_rate\n",
    "        self.previous_values = []\n",
    "        self.delta_weights = []\n",
    "        self.Neuron_Index = neuron_index\n",
    "        for i in range(number_neurons_in_previous_layer):\n",
    "            self.Weights.append(random.uniform(-initialization_value, initialization_value))\n",
    "        self.Weights.append(random.uniform(-initialization_value, initialization_value))\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        self.Weights = weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "\n",
    "Forward Propagation is the algorithm that takes your input and pushes through the neural network layers/neurons to create one or more output values.  Recall from the last lesson that each neuron calculates it's output as the weighted sum of the neuron's input vector and the weights.  In other words, given:\n",
    "$\\vec{W} = Weight\\ vector$\n",
    "\n",
    "$\\vec{I} = Input\\ vector$\n",
    "\n",
    "$\\theta = Bias$\n",
    "\n",
    "Here is what our output looks like without an activation function\n",
    "\n",
    "$output = \\vec{W}\\cdot\\vec{I} - \\theta$\n",
    "\n",
    "The activation function transforms the output of the weighted sum.  A common activation function is the sigmoid function, given by \n",
    "\n",
    "$y^{sigmoid}=1/{(1+e^{-X})}$\n",
    "\n",
    "A benefit of the sigmoid function for activation is that it forces the output to be bounded between 0 and 1.  I highly recommend you graph out the sigmoid function so you are aware of how it looks.  Why do you think this form would be of use?\n",
    "\n",
    "A key attribute of any activation function is that it is differentiable.  As we will see, it's differntiability allows us to determine how to distribute the error.  For the Sigmoid function the derivative is $x*(1-x)$\n",
    "\n",
    "For these lessons we will be implementing any class for activation function using two methods:  \n",
    "<code>activation_function(value)</code>\n",
    "and\n",
    "<code>activation_function_derivative(result)</code>\n",
    "\n",
    "The code below implements the sigmoid activation function and its derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidActivation:\n",
    "\n",
    "    @staticmethod\n",
    "    def activation_function(value):\n",
    "        return 1 / (1 + math.exp(-value))\n",
    "\n",
    "    @staticmethod\n",
    "    def activation_function_derivative(result):\n",
    "        return result * (1.0 - result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, given the sigmoid function activation function we will now build the forward propagation method.  The below code is complete except for the calculations to compute output vector (weighted sum of the values and weights including the bias) and the result of the activation function.  Complete the below function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(network, values, activation):\n",
    "    \"\"\"\n",
    "    Runs the forward propagation algorithm\n",
    "    \"\"\"\n",
    "    results = values\n",
    "    for layer in network.Layers[1:]:\n",
    "        # initialize a vector for the layer's values (to be sent to next layer)\n",
    "        values_for_layer = []\n",
    "\n",
    "        for neuron in layer.Neurons:\n",
    "            # store the previous values for use in back propagation\n",
    "            neuron.previous_values = results\n",
    "            # reset the results\n",
    "            neuron.Result = 0\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            # calculate the weighted sum of the weights by the results\n",
    "           \n",
    "        \n",
    "        \n",
    "            # update the neuron.Result as the activation_function of the weighted sum\n",
    "            \n",
    "            \n",
    "            # END YOUR CODE HERE \n",
    "            # add the new output value to the values vector for the current layer\n",
    "            values_for_layer.append(neuron.Result)\n",
    "        results = values_for_layer\n",
    "\n",
    "    return values_for_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the forward propagation method is complete each neuron will have the vector of the results from the previous layer as well as an output.  One of the cool aspects of a neural network is that once the weights have been trained the algorithm only needs to run the forward propagation method to calculate the resulting output of the network.  Then, based on a threshold the inputted vector is classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation Algorithm\n",
    "\n",
    "Take a look over the following video to get an idea what is going on with backprop: https://youtu.be/Ilg3gGewQ5U\n",
    "\n",
    "Backprop effectively provides a way of distributing the error across all of the weights based on their contribution.  To help walk through the math, let's unravel the equations presented in Negnivitski's discussion.\n",
    "\n",
    "We will start with the simple error function:\n",
    "\n",
    "$e_k(p) = y_{d,k}(p) - y_k(p)$\n",
    "\n",
    "This isn't anything crazy. All we are saying is that the error is the actual output subtracted with what we predicted.\n",
    "\n",
    "Next we seek to describe to find how we descibe the gradient for each neuron in the output layer. Well this is simply the derivative of the activation function multipled by the error gradient.\n",
    "\n",
    "Remember the activation function is what we pass our final weighted sum to\n",
    "\n",
    "$output = activationfunction(\\vec{W}\\cdot\\vec{I} - \\theta)$\n",
    "\n",
    "As the gradient is pretty boring for a linear function lets suppose we have a sigmoid activation function where .\n",
    "\n",
    "$sigmoid(x) = \\frac{e^x}{1 + e^x}$\n",
    "\n",
    "\n",
    "Now the gradient for this function is\n",
    "\n",
    "$\\delta_k(p) = y_k(p)[1-y_k(p)]e_k(p)$\n",
    "\n",
    "Finally, the final change in weight correction for connection j,k is\n",
    "\n",
    "$\\delta_{j,k} = \\alpha \\ y_j(p) \\ \\delta_k(p)$\n",
    "\n",
    "Where alpha is our learning rate (you have seen this before) $y_j$ is the output at the neuron in layer j with weight j,k and the error gradient is just what we calculated.\n",
    "\n",
    "Finally, the new weight is \n",
    "\n",
    "$new\\ weight = old\\ weight\\ +\\ the\\ change\\ in\\ weight$\n",
    "\n",
    "For the hidden layers the equation is slightly different we have \n",
    "\n",
    "$\\delta_j(p) = y_j(p) \\ (1-y_j(p)) \\ \\sum_{k = 1}^{j} {\\delta_k(p)w_{jk}(p)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the equations, it's time to implement!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagate(network, output_values, correct_values, activation):\n",
    "    def layer_bp(layer, errors, activation):\n",
    "        '''\n",
    "        Takes the errors from the previous layer to update the weights\n",
    "        :param layer:  the layer being updated\n",
    "        :param errors:  errors calculated from the previous layer\n",
    "        :param activation:  the activation function class\n",
    "        :return:  updated errors for the next layer\n",
    "        '''\n",
    "        error_vector = []\n",
    "        for (neuron, error) in zip(layer.Neurons, errors):\n",
    "            # calculate each neurons error vector\n",
    "            error_vector_from_neuron = neuron_back_propagate(neuron,\n",
    "                                                             error, activation)\n",
    "            if len(error_vector) == 0:\n",
    "                for i in range(len(error_vector_from_neuron)):\n",
    "                    error_vector.append(0)\n",
    "            for i in range(len(error_vector_from_neuron)):\n",
    "                error_vector[i] += error_vector_from_neuron[i]\n",
    "        return error_vector\n",
    "\n",
    "    def neuron_back_propagate(neuron, error, activation):\n",
    "        \"\"\"\n",
    "        Calculates weight updates for a neuron through back prop\n",
    "        :param neuron:  the neuron being evaluated\n",
    "        :param error:  the error associated with this neuron\n",
    "        :param activation:  the activation function being used\n",
    "        :return: the error_vector for the \n",
    "        \"\"\"\n",
    "        derivative = activation.activation_function_derivative(neuron.Result)\n",
    "        error_gradient = error * derivative\n",
    "        \n",
    "        # calculate the change in weights using the error gradient\n",
    "        neuron.delta_weights.clear()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        for i in range(len(neuron.previous_values)):\n",
    "            delta = 0  # FIX THIS\n",
    "            neuron.delta_weights.append(delta)\n",
    "        neuron.delta_weights.append(0) # FIX THIS\n",
    "        \n",
    "        # YOUR CODE COMPLETE\n",
    "        \n",
    "        # The error vector becomes the weights modified by the error gradient\n",
    "        error_vector = []\n",
    "        for weight in neuron.Weights:\n",
    "            error_vector.append(weight * error_gradient)\n",
    "        return error_vector\n",
    "\n",
    "    errors = []\n",
    "    # Calculate the error of the prediction at the output layer\n",
    "    for (output, correct) in zip(output_values, correct_values):\n",
    "        errors.append(correct - output)\n",
    "\n",
    "    # move through hidden layers to calculate the error (move from output to input)\n",
    "    for reversed_layer in reversed(network.Layers[1:]):\n",
    "        errors = layer_bp(reversed_layer, errors, activation)\n",
    "\n",
    "    # Update the weights based on the delta calculated\n",
    "    for layer in network.Layers[1:]:\n",
    "        for neuron in layer.Neurons:\n",
    "            for i in range(len(neuron.Weights)):\n",
    "                neuron.Weights[i] += neuron.delta_weights[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this functionality using the calculations given in Negnivitsky.  Below is a test routine for your to compare the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_negnivitsky():\n",
    "    init_value = 2.4/2  # per Haykin set the init to 2.4/<number input neurons>\n",
    "\n",
    "    network = Network([2, 2, 1], init_value, 0.1)\n",
    "    activation = SigmoidActivation()\n",
    "    network.set_weights(1, 0, [0.5, 0.4, 0.8])  # layer #, neuron #, weights + bias\n",
    "    network.set_weights(1, 1, [0.9, 1.0, -0.1])\n",
    "    network.set_weights(2, 0, [-1.2, 1.1, 0.3])\n",
    "\n",
    "    result = forward_propagate(network, [1, 1], activation)\n",
    "    print(\"result:\")\n",
    "    print(result)\n",
    "    print(\"\\nnetwork:\")\n",
    "    print_network(network)\n",
    "\n",
    "    # Neuron 3 from Negnivitski diagram\n",
    "    assert(abs(network.Layers[1].Neurons[0].Result - 0.5249791874789399) < 0.0001)\n",
    "    assert(abs(network.Layers[1].Neurons[1].Result - 0.8807970779778823) < 0.0001)\n",
    "    assert(abs(network.Layers[2].Neurons[0].Result - 0.5097242138886783) < 0.0001)\n",
    "\n",
    "    print(\"\\n-- BackProp --\")\n",
    "    backward_propagate(network, result, [0], activation)\n",
    "    print_network(network)\n",
    "\n",
    "    # Neuron 3 from Negnivitski diagram\n",
    "    assert(abs(network.Layers[1].Neurons[0].Weights[0] - 0.5038119477996761) < 0.0001)\n",
    "    assert(abs(network.Layers[1].Neurons[0].Weights[1] - 0.40381194779967616) < 0.0001)\n",
    "    assert(abs(network.Layers[1].Neurons[0].Weights[2] - 0.796188052200324) < 0.0001)\n",
    "    # Neuron 4 from Negnivitski diagram\n",
    "    assert(abs(network.Layers[1].Neurons[1].Weights[0] - 0.89852881792090521) < 0.0001)\n",
    "    assert(abs(network.Layers[1].Neurons[1].Weights[1] - 0.9985288179209052) < 0.0001)\n",
    "    assert(abs(network.Layers[1].Neurons[1].Weights[2] - -0.09852881792090515) < 0.0001)\n",
    "    # Neuron 5 from Negnivitski diagram\n",
    "    assert(abs(network.Layers[2].Neurons[0].Weights[0] - -1.2066873347075837) < 0.0001)\n",
    "    assert(abs(network.Layers[2].Neurons[0].Weights[1] - 1.0887801554606653) < 0.0001)\n",
    "    assert(abs(network.Layers[2].Neurons[0].Weights[2] - 0.3127382853779363) < 0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:\n",
      "[0]\n",
      "\n",
      "network:\n",
      "Neuron_Index: [Weights] => Neuron_Output\n",
      "\n",
      "Layer 0 - Input\n",
      "   Neuron 0: [-0.8509993546018211] => 0\n",
      "   Neuron 1: [0.9718916833623055] => 0\n",
      "Layer 1 - Hidden\n",
      "   Neuron 0: [0.5, 0.4, 0.8] => 0\n",
      "   Neuron 1: [0.9, 1.0, -0.1] => 0\n",
      "Layer 2 - Output\n",
      "   Neuron 0: [-1.2, 1.1, 0.3] => 0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-37cfbc45d577>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_negnivitsky\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-674fc139681b>\u001b[0m in \u001b[0;36mtest_negnivitsky\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Neuron 3 from Negnivitski diagram\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNeurons\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResult\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m0.5249791874789399\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNeurons\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResult\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m0.8807970779778823\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNeurons\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResult\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m0.5097242138886783\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.0001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_negnivitsky()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want answers for the above questions/problems, below is the code I used:\n",
    "\n",
    "Answer for Forward Prop:\n",
    "<code>\n",
    "            # calculate the weighted sum of the weights by the results\n",
    "            for i in range(len(results)):\n",
    "                neuron.Result += neuron.Weights[i] * results[i]\n",
    "            neuron.Result += -1.0 * neuron.Weights[-1]\n",
    "            \n",
    "            # update the result using the activation function\n",
    "            neuron.Result = activation.activation_function(neuron.Result)\n",
    "</code>\n",
    "\n",
    "Answer for BackProp:\n",
    "<code>\n",
    "   for i in range(len(neuron.previous_values)):\n",
    "            delta = network.Learning_Rate * error_gradient * neuron.previous_values[i]\n",
    "            neuron.delta_weights.append(delta)\n",
    "        neuron.delta_weights.append(network.Learning_Rate * error_gradient * -1.0)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
